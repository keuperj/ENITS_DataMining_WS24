{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_RORTmNzkYx"
   },
   "source": [
    "# Data Mining for Security Applications: final project\n",
    "\n",
    "This final hands on project will count for 50% of your final grade (exam is the other 50%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8F4RGo-zkY4"
   },
   "source": [
    "## Organization\n",
    "* you can start the project as of now\n",
    "* the project has to be handed in no later than March 1st - NO EXTENSIONS!\n",
    "\n",
    "### Project Submission \n",
    "* hand in a single jupyter notebook for your project (via moodle)\n",
    "* set all paths as relative paths such that the data is in the same folder as the notebook\n",
    "* your notebook should run without errors (uncomment parts that do not work)\n",
    "* Use markdown cells and comments in the code to document and motivate you solution\n",
    "* show and analyze intermediate results\n",
    "* evaluate and discuss your solution\n",
    "\n",
    "### Grading Criteria\n",
    "* 4.0 : notebook that works and gives some solution to the problem\n",
    "* 3.0 : + good documentation, evaluation and discussion\n",
    "* 2.0 : + complete processing pipeline, good results\n",
    "* 1.0 : + very detailed documentation and analysis, hyper-parameter optimization, tried and compared more than one method \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59FQDpGQzkY4"
   },
   "source": [
    "# Task: Breaking image captchas  \n",
    "\n",
    "We are working on this Kaggle Challenge: https://www.kaggle.com/fournierp/captcha-version-2-images  \n",
    "\n",
    "HINT: have a good look at the problem description and the notebooks of other users to get started \n",
    "\n",
    "* Train and test images are in the according folders\n",
    "* The true labels are encoded in the file names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqupLDr6zkY5"
   },
   "source": [
    "## Tasks to follow:\n",
    "* write code to read the images [use the imageio lib](https://imageio.readthedocs.io/en/stable/userapi.html) and convert then into NUMPY feature vectors\n",
    "and labels\n",
    "    * HINT: use [the os lib](https://docs.python.org/2/library/os.html#os.listdir) to get files in a directory\n",
    "* the hardest part is to solve the segmentation problem: splitting the image into single characters\n",
    "    * try Clustering over pixel positions\n",
    "    * or a density projection along the y-axis\n",
    "* crate a training data set of labeled character segments\n",
    "    * evaluate this step\n",
    "* train a CNN or MLP network to classify character segments\n",
    "    * evaluate this step\n",
    "* build a full pipeline to transform capcha image inputs into strings\n",
    "* Evaluate and discuss your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1612726399445,
     "user": {
      "displayName": "Melanie McLeod",
      "photoUrl": "",
      "userId": "15020505036329731243"
     },
     "user_tz": -60
    },
    "id": "p2pjwAQdzkY5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/student/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#Pip installs for all packages if needed\n",
    "#!pip install os\n",
    "#!pip install imageio\n",
    "#!pip install opencv-python\n",
    "#!pip install matplotlib\n",
    "#!pip install tensorflow\n",
    "#!pip install keras\n",
    "\n",
    "\n",
    "#Importing all needed packages\n",
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "from cv2 import cv2\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import keras\n",
    "\n",
    "#Import Modeling Packages\n",
    "from keras.layers import Layer, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras import backend as K\n",
    "from keras import layers\n",
    "from keras.models import Model, Sequential, load_model \n",
    "from keras import callbacks\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLgI9t33zkY6"
   },
   "source": [
    "First visualization of data set and how with importing in with imageio the images apppears in plots. This helps distingish if the image is brought in as indiviual letter/numbers or the full image is stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "executionInfo": {
     "elapsed": 710,
     "status": "error",
     "timestamp": 1612726405693,
     "user": {
      "displayName": "Melanie McLeod",
      "photoUrl": "",
      "userId": "15020505036329731243"
     },
     "user_tz": -60
    },
    "id": "E-4IegGGzkY7",
    "outputId": "4a58a8be-2a34-4007-b146-f83c4da25494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 200)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example of how imageio reads the data in visual form. the Captcha is in a clear dark purple. \n",
    "image = imageio.imread('../Project/train/2b827.png', as_gray=True)\n",
    "print(image.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image, cmap=plt.get_cmap('gray'))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lsSRmQJzkY7"
   },
   "source": [
    "After we know how the image comes in we can set up a function to import and store all the images for testing or training sets. According to the Kaggle information all images are in 50 by 200 PNGS. Also found online are the symbols avaible to Captcha, which we can utilized for ensure the y match up with the limited options for Captcha. Having a limit of matches makes the match processing easier. Note: we utilized `as_gray=True` to indicate the images are in grey scale this reduces our shape from a (5,200,4) to (5,200) which aligns with the size of images perfectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l-lCMQtxzkY7"
   },
   "outputs": [],
   "source": [
    "#Init main values\n",
    "symbols = string.ascii_lowercase + \"0123456789\" # All symbols captcha can contain\n",
    "num_symbols = len(symbols)\n",
    "img_shape = (50, 200, 1)\n",
    "\n",
    "#preprocessing the data into Numpy array's\n",
    "def process_image(type):\n",
    "    n_samples = len(os.listdir(type))\n",
    "    X = np.zeros((n_samples, 50, 200, 1)) # the size of image is known to be 200 x 50 PNGs  and we have # of samples\n",
    "    y = np.zeros((5, n_samples, num_symbols)) #5*970*36 shape\n",
    "\n",
    "    for i,pic in enumerate(os.listdir(type)):\n",
    "        # Read image as grayscale\n",
    "        img = cv2.imread(os.path.join(type,pic), cv2.IMREAD_GRAYSCALE)\n",
    "        pic_target = pic[:-4]\n",
    "        \n",
    "        #All Captcha Images are assumed to be length of 5 characters\n",
    "        if len(pic_target) < 6:\n",
    "            \n",
    "            # Scale and reshape image\n",
    "            img = img / 255.0\n",
    "            img = np.reshape(img, (50, 200, 1))\n",
    "            \n",
    "            # Define targets based on file name and code them using OneHotEncoding\n",
    "            targs = np.zeros((5, num_symbols))\n",
    "            \n",
    "            for j, l in enumerate(pic_target):\n",
    "                ind = symbols.find(l)\n",
    "                targs[j, ind] = 1\n",
    "             \n",
    "            #Write the pixels (encoded image) to X and the OneHotEncoded Label to y\n",
    "            X[i] = img\n",
    "            y[:, i] = targs\n",
    "    \n",
    "    # Return final data\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUGeyLTvzkY8"
   },
   "source": [
    "Below we utlize the relative paths of train and test folders to set up the X,y datasets. Stored in process_data() is our function for utilizing cv2.imread lib  (same to imageio, but had less conflicts) for storing the data in vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8mfIVwz0zkY8",
    "outputId": "4c80b306-9860-4f32-ee64-fc50486680b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of X: train (970, 50, 200, 1) test (100, 50, 200, 1)\n",
      "Shapes of y: train (5, 970, 36) test (5, 100, 36)\n"
     ]
    }
   ],
   "source": [
    "#Create Train and Test sets\n",
    "train = os.getcwd()+'/train'\n",
    "test = os.getcwd()+'/test'\n",
    "\n",
    "X_train, y_train = process_image(train)\n",
    "X_test, y_test = process_image(test)\n",
    "\n",
    "print(\"Shapes of X: train\", X_train.shape, \"test\", X_test.shape)\n",
    "print(\"Shapes of y: train\", y_train.shape, \"test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6A5CBbgzkY_"
   },
   "source": [
    "Now that the data stored in the X,y for train and test we can create a CNN model. We are utilizing a CNN model because they were designed for image mapping. https://machinelearningmastery.com/when-to-use-mlp-cnn-and-rnn-neural-networks/. In creating the model we need to breadown the image from a whole view to try to get section of each letter/number for the model to match. Since we know there are only 5 characters in the captcha images we can use this for setting up 5 branches for the CNN. The activation function of relu was selected  in betwenn layers as this is a pretty standard starting point and is more computationally effiecent. Since the ultimate goal would be to accurately predict to break the captcha the evaluation metric being used is overall accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UZq16J_4zkY_"
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    image = layers.Input(shape=img_shape) # Get image as an input and process it through some Convs\n",
    "    conv1 = layers.Conv2D(16, (3, 3), padding='same', activation='relu')(image)\n",
    "    pool1 = layers.MaxPooling2D(padding='same')(conv1)  # 100x25\n",
    "    conv2 = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(pool1)\n",
    "    pool2 = layers.MaxPooling2D(padding='same')(conv2)  # 50x13\n",
    "    conv3 = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(pool2)\n",
    "    bn = layers.BatchNormalization()(conv3)\n",
    "    pool3 = layers.MaxPooling2D(padding='same')(bn)  # 25x7\n",
    "    \n",
    "    # Get flattened vector and make 5 branches from it. Each branch will predict one letter\n",
    "    flat = layers.Flatten()(pool3)\n",
    "    outputs = []\n",
    "    for _ in range(5):\n",
    "        dens1 = layers.Dense(64, activation='relu')(flat)\n",
    "        drop = layers.Dropout(0.5)(dens1)\n",
    "        res = layers.Dense(num_symbols, activation='sigmoid')(drop)\n",
    "\n",
    "        outputs.append(res)\n",
    "    \n",
    "    # Compile model and return it\n",
    "    model = Model(image, outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kihIontFzkZA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Setup the Model, History Logs \n",
    "model = create_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/student/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 776 samples, validate on 194 samples\n",
      "Epoch 1/30\n",
      "776/776 [==============================] - 8s 11ms/step - loss: 17.0997 - dense_2_loss: 3.3460 - dense_4_loss: 3.4431 - dense_6_loss: 3.4145 - dense_8_loss: 3.5081 - dense_10_loss: 3.3687 - dense_2_accuracy: 0.0515 - dense_4_accuracy: 0.0477 - dense_6_accuracy: 0.0593 - dense_8_accuracy: 0.0438 - dense_10_accuracy: 0.0593 - val_loss: 17.7554 - val_dense_2_loss: 3.7430 - val_dense_4_loss: 3.5103 - val_dense_6_loss: 3.4980 - val_dense_8_loss: 3.5357 - val_dense_10_loss: 3.4912 - val_dense_2_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0670 - val_dense_6_accuracy: 0.0412 - val_dense_8_accuracy: 0.0464 - val_dense_10_accuracy: 0.1237\n",
      "Epoch 2/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 15.9960 - dense_2_loss: 3.0711 - dense_4_loss: 3.2782 - dense_6_loss: 3.1512 - dense_8_loss: 3.2808 - dense_10_loss: 3.1994 - dense_2_accuracy: 0.0747 - dense_4_accuracy: 0.0464 - dense_6_accuracy: 0.0631 - dense_8_accuracy: 0.0567 - dense_10_accuracy: 0.0876 - val_loss: 17.5551 - val_dense_2_loss: 3.8329 - val_dense_4_loss: 3.4308 - val_dense_6_loss: 3.4149 - val_dense_8_loss: 3.4730 - val_dense_10_loss: 3.4248 - val_dense_2_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0670 - val_dense_6_accuracy: 0.1237 - val_dense_8_accuracy: 0.0412 - val_dense_10_accuracy: 0.1237\n",
      "Epoch 3/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 15.2635 - dense_2_loss: 2.8925 - dense_4_loss: 3.1149 - dense_6_loss: 3.0135 - dense_8_loss: 3.1620 - dense_10_loss: 3.0896 - dense_2_accuracy: 0.0747 - dense_4_accuracy: 0.0670 - dense_6_accuracy: 0.0851 - dense_8_accuracy: 0.0683 - dense_10_accuracy: 0.1082 - val_loss: 17.4382 - val_dense_2_loss: 3.9728 - val_dense_4_loss: 3.3597 - val_dense_6_loss: 3.3874 - val_dense_8_loss: 3.4108 - val_dense_10_loss: 3.3469 - val_dense_2_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0670 - val_dense_6_accuracy: 0.0773 - val_dense_8_accuracy: 0.0412 - val_dense_10_accuracy: 0.1237\n",
      "Epoch 4/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 14.5661 - dense_2_loss: 2.7462 - dense_4_loss: 2.9706 - dense_6_loss: 2.8694 - dense_8_loss: 3.0162 - dense_10_loss: 2.9434 - dense_2_accuracy: 0.1070 - dense_4_accuracy: 0.0799 - dense_6_accuracy: 0.1250 - dense_8_accuracy: 0.0954 - dense_10_accuracy: 0.0954 - val_loss: 17.4779 - val_dense_2_loss: 3.8834 - val_dense_4_loss: 3.3995 - val_dense_6_loss: 3.3947 - val_dense_8_loss: 3.4285 - val_dense_10_loss: 3.3925 - val_dense_2_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.1031 - val_dense_6_accuracy: 0.1856 - val_dense_8_accuracy: 0.0979 - val_dense_10_accuracy: 0.0258\n",
      "Epoch 5/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 13.9931 - dense_2_loss: 2.6552 - dense_4_loss: 2.8491 - dense_6_loss: 2.7395 - dense_8_loss: 2.8882 - dense_10_loss: 2.8584 - dense_2_accuracy: 0.1327 - dense_4_accuracy: 0.1263 - dense_6_accuracy: 0.1572 - dense_8_accuracy: 0.1108 - dense_10_accuracy: 0.1044 - val_loss: 17.4419 - val_dense_2_loss: 3.6887 - val_dense_4_loss: 3.4458 - val_dense_6_loss: 3.4360 - val_dense_8_loss: 3.4280 - val_dense_10_loss: 3.4294 - val_dense_2_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.1701 - val_dense_6_accuracy: 0.1753 - val_dense_8_accuracy: 0.2268 - val_dense_10_accuracy: 0.1340\n",
      "Epoch 6/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 13.4662 - dense_2_loss: 2.5325 - dense_4_loss: 2.7802 - dense_6_loss: 2.5964 - dense_8_loss: 2.7953 - dense_10_loss: 2.7646 - dense_2_accuracy: 0.1856 - dense_4_accuracy: 0.1366 - dense_6_accuracy: 0.2358 - dense_8_accuracy: 0.1392 - dense_10_accuracy: 0.1289 - val_loss: 17.4284 - val_dense_2_loss: 3.7011 - val_dense_4_loss: 3.4723 - val_dense_6_loss: 3.3788 - val_dense_8_loss: 3.4481 - val_dense_10_loss: 3.4102 - val_dense_2_accuracy: 0.0000e+00 - val_dense_4_accuracy: 0.0928 - val_dense_6_accuracy: 0.2010 - val_dense_8_accuracy: 0.2010 - val_dense_10_accuracy: 0.1753\n",
      "Epoch 7/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 12.8817 - dense_2_loss: 2.4026 - dense_4_loss: 2.6269 - dense_6_loss: 2.4583 - dense_8_loss: 2.7279 - dense_10_loss: 2.6670 - dense_2_accuracy: 0.1765 - dense_4_accuracy: 0.1830 - dense_6_accuracy: 0.2410 - dense_8_accuracy: 0.1907 - dense_10_accuracy: 0.1559 - val_loss: 16.8344 - val_dense_2_loss: 3.9703 - val_dense_4_loss: 3.2418 - val_dense_6_loss: 3.1567 - val_dense_8_loss: 3.2747 - val_dense_10_loss: 3.1898 - val_dense_2_accuracy: 0.0052 - val_dense_4_accuracy: 0.2474 - val_dense_6_accuracy: 0.2680 - val_dense_8_accuracy: 0.3041 - val_dense_10_accuracy: 0.1959\n",
      "Epoch 8/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 12.0543 - dense_2_loss: 2.2350 - dense_4_loss: 2.4356 - dense_6_loss: 2.2617 - dense_8_loss: 2.5652 - dense_10_loss: 2.5346 - dense_2_accuracy: 0.2307 - dense_4_accuracy: 0.2242 - dense_6_accuracy: 0.2706 - dense_8_accuracy: 0.2101 - dense_10_accuracy: 0.1675 - val_loss: 16.4601 - val_dense_2_loss: 4.4127 - val_dense_4_loss: 2.9963 - val_dense_6_loss: 2.9705 - val_dense_8_loss: 3.0602 - val_dense_10_loss: 3.0021 - val_dense_2_accuracy: 0.0206 - val_dense_4_accuracy: 0.1598 - val_dense_6_accuracy: 0.2474 - val_dense_8_accuracy: 0.2474 - val_dense_10_accuracy: 0.1289\n",
      "Epoch 9/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 11.4074 - dense_2_loss: 2.0002 - dense_4_loss: 2.2932 - dense_6_loss: 2.1326 - dense_8_loss: 2.5033 - dense_10_loss: 2.4564 - dense_2_accuracy: 0.2861 - dense_4_accuracy: 0.2822 - dense_6_accuracy: 0.2912 - dense_8_accuracy: 0.2539 - dense_10_accuracy: 0.2126 - val_loss: 16.3941 - val_dense_2_loss: 3.9966 - val_dense_4_loss: 3.0826 - val_dense_6_loss: 2.9682 - val_dense_8_loss: 3.1778 - val_dense_10_loss: 3.0796 - val_dense_2_accuracy: 0.0155 - val_dense_4_accuracy: 0.3247 - val_dense_6_accuracy: 0.2990 - val_dense_8_accuracy: 0.3660 - val_dense_10_accuracy: 0.2320\n",
      "Epoch 10/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 10.3915 - dense_2_loss: 1.7678 - dense_4_loss: 2.0412 - dense_6_loss: 1.9662 - dense_8_loss: 2.3313 - dense_10_loss: 2.2501 - dense_2_accuracy: 0.4034 - dense_4_accuracy: 0.3557 - dense_6_accuracy: 0.3325 - dense_8_accuracy: 0.2500 - dense_10_accuracy: 0.2371 - val_loss: 15.7174 - val_dense_2_loss: 4.4034 - val_dense_4_loss: 2.7738 - val_dense_6_loss: 2.6920 - val_dense_8_loss: 2.9351 - val_dense_10_loss: 2.8809 - val_dense_2_accuracy: 0.0412 - val_dense_4_accuracy: 0.3866 - val_dense_6_accuracy: 0.3608 - val_dense_8_accuracy: 0.2938 - val_dense_10_accuracy: 0.2887\n",
      "Epoch 11/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 9.2880 - dense_2_loss: 1.4799 - dense_4_loss: 1.7650 - dense_6_loss: 1.7193 - dense_8_loss: 2.2188 - dense_10_loss: 2.1075 - dense_2_accuracy: 0.5322 - dense_4_accuracy: 0.4601 - dense_6_accuracy: 0.3853 - dense_8_accuracy: 0.3080 - dense_10_accuracy: 0.2822 - val_loss: 15.2309 - val_dense_2_loss: 4.8673 - val_dense_4_loss: 2.4740 - val_dense_6_loss: 2.3957 - val_dense_8_loss: 2.7793 - val_dense_10_loss: 2.5955 - val_dense_2_accuracy: 0.0515 - val_dense_4_accuracy: 0.6134 - val_dense_6_accuracy: 0.4536 - val_dense_8_accuracy: 0.4330 - val_dense_10_accuracy: 0.3093\n",
      "Epoch 12/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 8.1680 - dense_2_loss: 1.1070 - dense_4_loss: 1.5136 - dense_6_loss: 1.5400 - dense_8_loss: 2.0456 - dense_10_loss: 1.9024 - dense_2_accuracy: 0.6379 - dense_4_accuracy: 0.5412 - dense_6_accuracy: 0.4781 - dense_8_accuracy: 0.3570 - dense_10_accuracy: 0.3299 - val_loss: 14.8044 - val_dense_2_loss: 4.8708 - val_dense_4_loss: 2.3439 - val_dense_6_loss: 2.3845 - val_dense_8_loss: 2.6593 - val_dense_10_loss: 2.4015 - val_dense_2_accuracy: 0.0464 - val_dense_4_accuracy: 0.6649 - val_dense_6_accuracy: 0.5258 - val_dense_8_accuracy: 0.4691 - val_dense_10_accuracy: 0.3814\n",
      "Epoch 13/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 7.1204 - dense_2_loss: 0.9135 - dense_4_loss: 1.1872 - dense_6_loss: 1.3392 - dense_8_loss: 1.9029 - dense_10_loss: 1.7462 - dense_2_accuracy: 0.7216 - dense_4_accuracy: 0.6250 - dense_6_accuracy: 0.5760 - dense_8_accuracy: 0.3943 - dense_10_accuracy: 0.3918 - val_loss: 14.1248 - val_dense_2_loss: 5.2913 - val_dense_4_loss: 1.9376 - val_dense_6_loss: 2.0839 - val_dense_8_loss: 2.4111 - val_dense_10_loss: 2.1794 - val_dense_2_accuracy: 0.0515 - val_dense_4_accuracy: 0.6907 - val_dense_6_accuracy: 0.5876 - val_dense_8_accuracy: 0.6134 - val_dense_10_accuracy: 0.5464\n",
      "Epoch 14/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 5.9903 - dense_2_loss: 0.6990 - dense_4_loss: 0.9891 - dense_6_loss: 1.1403 - dense_8_loss: 1.6132 - dense_10_loss: 1.5124 - dense_2_accuracy: 0.7745 - dense_4_accuracy: 0.6933 - dense_6_accuracy: 0.6095 - dense_8_accuracy: 0.4678 - dense_10_accuracy: 0.4575 - val_loss: 13.7268 - val_dense_2_loss: 6.7087 - val_dense_4_loss: 1.4702 - val_dense_6_loss: 1.5860 - val_dense_8_loss: 1.9372 - val_dense_10_loss: 1.6038 - val_dense_2_accuracy: 0.0361 - val_dense_4_accuracy: 0.7835 - val_dense_6_accuracy: 0.6701 - val_dense_8_accuracy: 0.6598 - val_dense_10_accuracy: 0.5979\n",
      "Epoch 15/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 5.1835 - dense_2_loss: 0.5603 - dense_4_loss: 0.7882 - dense_6_loss: 0.9686 - dense_8_loss: 1.4868 - dense_10_loss: 1.3757 - dense_2_accuracy: 0.8028 - dense_4_accuracy: 0.7552 - dense_6_accuracy: 0.6869 - dense_8_accuracy: 0.5206 - dense_10_accuracy: 0.5052 - val_loss: 13.5144 - val_dense_2_loss: 6.8260 - val_dense_4_loss: 1.2833 - val_dense_6_loss: 1.5129 - val_dense_8_loss: 1.8948 - val_dense_10_loss: 1.6387 - val_dense_2_accuracy: 0.0515 - val_dense_4_accuracy: 0.8144 - val_dense_6_accuracy: 0.6753 - val_dense_8_accuracy: 0.6907 - val_dense_10_accuracy: 0.6649\n",
      "Epoch 16/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 4.4564 - dense_2_loss: 0.4588 - dense_4_loss: 0.6446 - dense_6_loss: 0.8426 - dense_8_loss: 1.2773 - dense_10_loss: 1.2301 - dense_2_accuracy: 0.8312 - dense_4_accuracy: 0.7642 - dense_6_accuracy: 0.7165 - dense_8_accuracy: 0.5863 - dense_10_accuracy: 0.5941 - val_loss: 13.5919 - val_dense_2_loss: 8.4133 - val_dense_4_loss: 0.8585 - val_dense_6_loss: 1.1408 - val_dense_8_loss: 1.5085 - val_dense_10_loss: 1.1997 - val_dense_2_accuracy: 0.0515 - val_dense_4_accuracy: 0.8299 - val_dense_6_accuracy: 0.7165 - val_dense_8_accuracy: 0.7268 - val_dense_10_accuracy: 0.7165\n",
      "Epoch 17/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 4.1662 - dense_2_loss: 0.4256 - dense_4_loss: 0.5561 - dense_6_loss: 0.8211 - dense_8_loss: 1.1940 - dense_10_loss: 1.1446 - dense_2_accuracy: 0.8698 - dense_4_accuracy: 0.8106 - dense_6_accuracy: 0.7204 - dense_8_accuracy: 0.5915 - dense_10_accuracy: 0.6082 - val_loss: 13.6611 - val_dense_2_loss: 8.3216 - val_dense_4_loss: 0.9238 - val_dense_6_loss: 1.2901 - val_dense_8_loss: 1.5292 - val_dense_10_loss: 1.2282 - val_dense_2_accuracy: 0.0412 - val_dense_4_accuracy: 0.8454 - val_dense_6_accuracy: 0.6804 - val_dense_8_accuracy: 0.7526 - val_dense_10_accuracy: 0.7371\n",
      "Epoch 18/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 3.6043 - dense_2_loss: 0.3363 - dense_4_loss: 0.5428 - dense_6_loss: 0.6698 - dense_8_loss: 1.0447 - dense_10_loss: 0.9982 - dense_2_accuracy: 0.8776 - dense_4_accuracy: 0.8312 - dense_6_accuracy: 0.8119 - dense_8_accuracy: 0.6340 - dense_10_accuracy: 0.6443 - val_loss: 13.9408 - val_dense_2_loss: 9.6649 - val_dense_4_loss: 0.6968 - val_dense_6_loss: 0.9601 - val_dense_8_loss: 1.0163 - val_dense_10_loss: 0.9441 - val_dense_2_accuracy: 0.0515 - val_dense_4_accuracy: 0.8454 - val_dense_6_accuracy: 0.7216 - val_dense_8_accuracy: 0.7835 - val_dense_10_accuracy: 0.7268\n",
      "Epoch 19/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 3.2770 - dense_2_loss: 0.3131 - dense_4_loss: 0.4566 - dense_6_loss: 0.6630 - dense_8_loss: 0.9082 - dense_10_loss: 0.9427 - dense_2_accuracy: 0.8943 - dense_4_accuracy: 0.8415 - dense_6_accuracy: 0.7784 - dense_8_accuracy: 0.6804 - dense_10_accuracy: 0.6546 - val_loss: 14.4345 - val_dense_2_loss: 10.8727 - val_dense_4_loss: 0.5484 - val_dense_6_loss: 0.8010 - val_dense_8_loss: 0.8873 - val_dense_10_loss: 0.7920 - val_dense_2_accuracy: 0.0515 - val_dense_4_accuracy: 0.8608 - val_dense_6_accuracy: 0.7474 - val_dense_8_accuracy: 0.7835 - val_dense_10_accuracy: 0.7938\n",
      "Epoch 20/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 2.9945 - dense_2_loss: 0.3013 - dense_4_loss: 0.3981 - dense_6_loss: 0.5648 - dense_8_loss: 0.8825 - dense_10_loss: 0.8353 - dense_2_accuracy: 0.8918 - dense_4_accuracy: 0.8647 - dense_6_accuracy: 0.8131 - dense_8_accuracy: 0.6778 - dense_10_accuracy: 0.7062 - val_loss: 15.6635 - val_dense_2_loss: 12.0184 - val_dense_4_loss: 0.5662 - val_dense_6_loss: 0.7736 - val_dense_8_loss: 0.8543 - val_dense_10_loss: 0.7486 - val_dense_2_accuracy: 0.0515 - val_dense_4_accuracy: 0.8247 - val_dense_6_accuracy: 0.7320 - val_dense_8_accuracy: 0.7629 - val_dense_10_accuracy: 0.7526\n",
      "Epoch 21/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 2.7173 - dense_2_loss: 0.2463 - dense_4_loss: 0.3848 - dense_6_loss: 0.5104 - dense_8_loss: 0.8585 - dense_10_loss: 0.7199 - dense_2_accuracy: 0.9124 - dense_4_accuracy: 0.8724 - dense_6_accuracy: 0.8235 - dense_8_accuracy: 0.7152 - dense_10_accuracy: 0.7371 - val_loss: 15.8069 - val_dense_2_loss: 12.2074 - val_dense_4_loss: 0.5527 - val_dense_6_loss: 0.8106 - val_dense_8_loss: 0.8299 - val_dense_10_loss: 0.6046 - val_dense_2_accuracy: 0.0515 - val_dense_4_accuracy: 0.8247 - val_dense_6_accuracy: 0.7371 - val_dense_8_accuracy: 0.7938 - val_dense_10_accuracy: 0.8299\n",
      "Epoch 22/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 2.4491 - dense_2_loss: 0.2347 - dense_4_loss: 0.3195 - dense_6_loss: 0.4711 - dense_8_loss: 0.7726 - dense_10_loss: 0.6688 - dense_2_accuracy: 0.9111 - dense_4_accuracy: 0.8995 - dense_6_accuracy: 0.8312 - dense_8_accuracy: 0.7204 - dense_10_accuracy: 0.7616 - val_loss: 15.7951 - val_dense_2_loss: 12.5137 - val_dense_4_loss: 0.4790 - val_dense_6_loss: 0.7486 - val_dense_8_loss: 0.8581 - val_dense_10_loss: 0.5936 - val_dense_2_accuracy: 0.0464 - val_dense_4_accuracy: 0.8505 - val_dense_6_accuracy: 0.7526 - val_dense_8_accuracy: 0.7629 - val_dense_10_accuracy: 0.8093\n",
      "Epoch 23/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 2.2996 - dense_2_loss: 0.2508 - dense_4_loss: 0.3091 - dense_6_loss: 0.4151 - dense_8_loss: 0.6870 - dense_10_loss: 0.6455 - dense_2_accuracy: 0.9124 - dense_4_accuracy: 0.8879 - dense_6_accuracy: 0.8686 - dense_8_accuracy: 0.7448 - dense_10_accuracy: 0.7642 - val_loss: 18.3986 - val_dense_2_loss: 15.3392 - val_dense_4_loss: 0.6580 - val_dense_6_loss: 0.8655 - val_dense_8_loss: 0.6714 - val_dense_10_loss: 0.6265 - val_dense_2_accuracy: 0.0515 - val_dense_4_accuracy: 0.7938 - val_dense_6_accuracy: 0.7423 - val_dense_8_accuracy: 0.8041 - val_dense_10_accuracy: 0.7990\n",
      "Epoch 24/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 2.1428 - dense_2_loss: 0.2320 - dense_4_loss: 0.2772 - dense_6_loss: 0.4227 - dense_8_loss: 0.6236 - dense_10_loss: 0.5819 - dense_2_accuracy: 0.9124 - dense_4_accuracy: 0.9111 - dense_6_accuracy: 0.8505 - dense_8_accuracy: 0.7577 - dense_10_accuracy: 0.7732 - val_loss: 18.7695 - val_dense_2_loss: 14.9777 - val_dense_4_loss: 1.0654 - val_dense_6_loss: 1.0948 - val_dense_8_loss: 1.0019 - val_dense_10_loss: 0.6792 - val_dense_2_accuracy: 0.0515 - val_dense_4_accuracy: 0.7680 - val_dense_6_accuracy: 0.6598 - val_dense_8_accuracy: 0.7784 - val_dense_10_accuracy: 0.7629\n",
      "Epoch 25/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 2.1155 - dense_2_loss: 0.1917 - dense_4_loss: 0.3059 - dense_6_loss: 0.4233 - dense_8_loss: 0.6464 - dense_10_loss: 0.5523 - dense_2_accuracy: 0.9278 - dense_4_accuracy: 0.8918 - dense_6_accuracy: 0.8479 - dense_8_accuracy: 0.7822 - dense_10_accuracy: 0.7951 - val_loss: 19.1170 - val_dense_2_loss: 15.4123 - val_dense_4_loss: 0.7633 - val_dense_6_loss: 1.1259 - val_dense_8_loss: 0.7862 - val_dense_10_loss: 0.7881 - val_dense_2_accuracy: 0.0412 - val_dense_4_accuracy: 0.8608 - val_dense_6_accuracy: 0.6856 - val_dense_8_accuracy: 0.7887 - val_dense_10_accuracy: 0.7526\n",
      "Epoch 26/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 1.9582 - dense_2_loss: 0.1970 - dense_4_loss: 0.2914 - dense_6_loss: 0.4256 - dense_8_loss: 0.5557 - dense_10_loss: 0.4981 - dense_2_accuracy: 0.9330 - dense_4_accuracy: 0.8969 - dense_6_accuracy: 0.8531 - dense_8_accuracy: 0.8054 - dense_10_accuracy: 0.8041 - val_loss: 18.3502 - val_dense_2_loss: 15.3224 - val_dense_4_loss: 0.6746 - val_dense_6_loss: 0.8099 - val_dense_8_loss: 0.6991 - val_dense_10_loss: 0.6308 - val_dense_2_accuracy: 0.0464 - val_dense_4_accuracy: 0.8196 - val_dense_6_accuracy: 0.7938 - val_dense_8_accuracy: 0.7835 - val_dense_10_accuracy: 0.8144\n",
      "Epoch 27/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 1.8693 - dense_2_loss: 0.1707 - dense_4_loss: 0.2664 - dense_6_loss: 0.3493 - dense_8_loss: 0.5366 - dense_10_loss: 0.5694 - dense_2_accuracy: 0.9291 - dense_4_accuracy: 0.9137 - dense_6_accuracy: 0.8711 - dense_8_accuracy: 0.8054 - dense_10_accuracy: 0.7964 - val_loss: 18.5341 - val_dense_2_loss: 15.2772 - val_dense_4_loss: 0.8151 - val_dense_6_loss: 0.8636 - val_dense_8_loss: 0.6939 - val_dense_10_loss: 0.6085 - val_dense_2_accuracy: 0.0412 - val_dense_4_accuracy: 0.8351 - val_dense_6_accuracy: 0.7680 - val_dense_8_accuracy: 0.8093 - val_dense_10_accuracy: 0.8196\n",
      "Epoch 28/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 1.7726 - dense_2_loss: 0.2043 - dense_4_loss: 0.2622 - dense_6_loss: 0.3290 - dense_8_loss: 0.5193 - dense_10_loss: 0.4715 - dense_2_accuracy: 0.9188 - dense_4_accuracy: 0.9046 - dense_6_accuracy: 0.8840 - dense_8_accuracy: 0.8106 - dense_10_accuracy: 0.8131 - val_loss: 18.1098 - val_dense_2_loss: 15.2409 - val_dense_4_loss: 0.6205 - val_dense_6_loss: 0.7699 - val_dense_8_loss: 0.6452 - val_dense_10_loss: 0.6406 - val_dense_2_accuracy: 0.0412 - val_dense_4_accuracy: 0.8454 - val_dense_6_accuracy: 0.7629 - val_dense_8_accuracy: 0.8144 - val_dense_10_accuracy: 0.8196\n",
      "Epoch 29/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 1.6928 - dense_2_loss: 0.1668 - dense_4_loss: 0.2181 - dense_6_loss: 0.3325 - dense_8_loss: 0.5092 - dense_10_loss: 0.4545 - dense_2_accuracy: 0.9459 - dense_4_accuracy: 0.9149 - dense_6_accuracy: 0.8737 - dense_8_accuracy: 0.8196 - dense_10_accuracy: 0.8273 - val_loss: 18.5065 - val_dense_2_loss: 15.4308 - val_dense_4_loss: 0.6735 - val_dense_6_loss: 0.8739 - val_dense_8_loss: 0.6785 - val_dense_10_loss: 0.5454 - val_dense_2_accuracy: 0.0412 - val_dense_4_accuracy: 0.8866 - val_dense_6_accuracy: 0.8041 - val_dense_8_accuracy: 0.8041 - val_dense_10_accuracy: 0.8299\n",
      "Epoch 30/30\n",
      "776/776 [==============================] - 5s 6ms/step - loss: 1.5547 - dense_2_loss: 0.1266 - dense_4_loss: 0.2269 - dense_6_loss: 0.2922 - dense_8_loss: 0.4692 - dense_10_loss: 0.4358 - dense_2_accuracy: 0.9433 - dense_4_accuracy: 0.9175 - dense_6_accuracy: 0.8943 - dense_8_accuracy: 0.8286 - dense_10_accuracy: 0.8325 - val_loss: 18.5134 - val_dense_2_loss: 15.4018 - val_dense_4_loss: 0.6139 - val_dense_6_loss: 0.9781 - val_dense_8_loss: 0.6861 - val_dense_10_loss: 0.5208 - val_dense_2_accuracy: 0.0515 - val_dense_4_accuracy: 0.8814 - val_dense_6_accuracy: 0.7423 - val_dense_8_accuracy: 0.7990 - val_dense_10_accuracy: 0.8505\n"
     ]
    }
   ],
   "source": [
    "#Model fitting\n",
    "hist = model.fit(X_train, [y_train[0],y_train[1], y_train[2], y_train[3], y_train[4]],\n",
    "                 batch_size=32,\n",
    "                 epochs=30, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/step\n",
      "Accuracy of Testing:  [6.905109825134278, 6.204279899597168, 0.945864737033844, 0.742710530757904, 0.6955447793006897, 0.659935712814331, 0.7799999713897705, 0.8700000047683716, 0.800000011920929, 0.8199999928474426, 0.8199999928474426]\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "score_test = model.evaluate(X_test,[y_test[0], y_test[1], y_test[2], y_test[3], y_test[4]],  verbose=1)\n",
    "                            \n",
    "print(\"Accuracy of Testing: \",score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrepretation of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of the test we can see that the model trained pretty well the 5 numbers give us the accuracy per letter. This averages for an overall ~82%. This is fairly accurate. Results may have been better with a lower validation size to allow for larger training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the model above is pretty good. It will still be wise to try out another model to see if we can make it even better. 1 option is to tune the parameters of the CNN also know as hyperparameterization. This can be done manually or through algothrims. Since we are dealing with less demsionality and more important pixel, I have taken a more manual approach. We noted above that there was also a line going through the image efforts to remove the line could also result in better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rework the preprocessing to for getting rid of the line through the characters\n",
    "kernel =np.ones((3,1),np.uint8)\n",
    "borderType = cv2.BORDER_CONSTANT\n",
    "def pad(src): \n",
    "    top = int(0.05 * src.shape[0])  # shape[0] = rows\n",
    "    bottom = top\n",
    "    left = int(0.15 * src.shape[1])  # shape[1] = cols\n",
    "    right = left\n",
    "    des=cv2.copyMakeBorder(src, top, bottom, left+1, right, borderType, None,255)\n",
    "    return cv2.bitwise_not(des)\n",
    "\n",
    "def process_v2(type):\n",
    "    \n",
    "    X=[]\n",
    "    y=[]\n",
    "    \n",
    "    for image in list(os.listdir(type)):\n",
    "        im=cv2.imread(os.path.join(type,image),0)\n",
    "        threshold=cv2.adaptiveThreshold(im, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY, 199, 5)\n",
    "        erosion =cv2.dilate(threshold,kernel,iterations=2)\n",
    "        s=str(image)\n",
    "        for i in range(5):\n",
    "            X.append(pad(erosion[:,(30+23*i):(30+23*(i+1))]))\n",
    "            y.append(s[-9+i])\n",
    "\n",
    "    X = np.reshape(np.array(X),(-1,54,30,1))\n",
    "    y = np.array(y)\n",
    "        \n",
    "    targ = []\n",
    "    l=['2','3','4','5','6','7','8','b','c','d','e','f','g','m','n','p','w','x','y']\n",
    "    for j in y:\n",
    "        i=l.index(j)\n",
    "        a=[]\n",
    "        for r in range(19):\n",
    "            if(r==i):\n",
    "                a.append(1)\n",
    "            else:\n",
    "                a.append(0)\n",
    "        a=np.array(a)\n",
    "        targ.append(a)\n",
    "    targ=np.array(targ) \n",
    "    y=np.array(targ) \n",
    "        \n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_v2, y_train_v2 = process_v2(train)\n",
    "X_test_v2, y_test_v2 = process_v2(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_v2():\n",
    "    model2 = Sequential()\n",
    "    model2.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n",
    "                     activation ='relu', input_shape = (54,30,1)))\n",
    "    model2.add(Conv2D(filters = 16, kernel_size = (5,5),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model2.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model2.add(Dropout(0.25))\n",
    "\n",
    "    model2.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model2.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model2.add(MaxPool2D(pool_size=(2,2)))\n",
    "    model2.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    model2.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model2.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                     activation ='relu'))\n",
    "    model2.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    model2.add(Dropout(0.25))\n",
    "\n",
    "    model2.add(Flatten())\n",
    "    model2.add(Dense(256, activation = \"relu\"))\n",
    "    model2.add(Dropout(0.5))\n",
    "    model2.add(Dense(19, activation = \"softmax\"))\n",
    "    \n",
    "    \n",
    "\n",
    "    optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model2.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    return model2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup learning rate for dropping out \n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                                patience=3, \n",
    "                                                verbose=1, \n",
    "                                                factor=0.5, \n",
    "                                              min_lr=0.00001)\n",
    "\n",
    "#Inspired from another Kaggle Notebook for augmenting the image for randomization in should increase accuracy, \n",
    "# as we create randomization in the data during modeling to deal with shifts in the characters\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = False, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "50/50 [==============================] - 20s 406ms/step - loss: 2.7625 - accuracy: 0.1447 - val_loss: 2.5729 - val_accuracy: 0.4619\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 2.1209 - accuracy: 0.3592 - val_loss: 1.1223 - val_accuracy: 0.7155\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 19s 387ms/step - loss: 1.5526 - accuracy: 0.5335 - val_loss: 0.6607 - val_accuracy: 0.8103\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 19s 387ms/step - loss: 1.1743 - accuracy: 0.6562 - val_loss: 0.6227 - val_accuracy: 0.8186\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 19s 386ms/step - loss: 0.8941 - accuracy: 0.7275 - val_loss: 0.3718 - val_accuracy: 0.8804\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 19s 387ms/step - loss: 0.7010 - accuracy: 0.7913 - val_loss: 0.3592 - val_accuracy: 0.8866\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 0.6029 - accuracy: 0.8240 - val_loss: 0.2098 - val_accuracy: 0.9423\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 22s 434ms/step - loss: 0.5116 - accuracy: 0.8441 - val_loss: 0.1924 - val_accuracy: 0.9340\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 21s 418ms/step - loss: 0.4514 - accuracy: 0.8579 - val_loss: 0.1551 - val_accuracy: 0.9361\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 21s 413ms/step - loss: 0.3903 - accuracy: 0.8843 - val_loss: 0.1586 - val_accuracy: 0.9485\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 19s 385ms/step - loss: 0.3699 - accuracy: 0.8925 - val_loss: 0.1318 - val_accuracy: 0.9464\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 19s 383ms/step - loss: 0.3411 - accuracy: 0.8998 - val_loss: 0.1066 - val_accuracy: 0.9588\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 19s 382ms/step - loss: 0.3124 - accuracy: 0.9133 - val_loss: 0.1590 - val_accuracy: 0.9361\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 19s 378ms/step - loss: 0.2823 - accuracy: 0.9225 - val_loss: 0.1160 - val_accuracy: 0.9546\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 19s 384ms/step - loss: 0.2748 - accuracy: 0.9167 - val_loss: 0.1005 - val_accuracy: 0.9608\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 20s 406ms/step - loss: 0.2549 - accuracy: 0.9222 - val_loss: 0.1033 - val_accuracy: 0.9588\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 21s 412ms/step - loss: 0.2415 - accuracy: 0.9248 - val_loss: 0.0901 - val_accuracy: 0.9670\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 19s 382ms/step - loss: 0.2098 - accuracy: 0.9370 - val_loss: 0.1303 - val_accuracy: 0.9608\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.2447 - accuracy: 0.9237 - val_loss: 0.0966 - val_accuracy: 0.9649\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.1991 - accuracy: 0.9385 - val_loss: 0.1232 - val_accuracy: 0.9546\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 19s 382ms/step - loss: 0.1717 - accuracy: 0.9481 - val_loss: 0.0933 - val_accuracy: 0.9567\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 19s 386ms/step - loss: 0.1480 - accuracy: 0.9591 - val_loss: 0.0753 - val_accuracy: 0.9711\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.1542 - accuracy: 0.9537 - val_loss: 0.0780 - val_accuracy: 0.9691\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.1278 - accuracy: 0.9612 - val_loss: 0.0741 - val_accuracy: 0.9753\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 19s 379ms/step - loss: 0.1426 - accuracy: 0.9584 - val_loss: 0.0775 - val_accuracy: 0.9691\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 19s 380ms/step - loss: 0.1470 - accuracy: 0.9591 - val_loss: 0.0775 - val_accuracy: 0.9691\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 19s 381ms/step - loss: 0.1532 - accuracy: 0.9595 - val_loss: 0.0673 - val_accuracy: 0.9753\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 19s 378ms/step - loss: 0.1222 - accuracy: 0.9634 - val_loss: 0.0626 - val_accuracy: 0.9773\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 19s 380ms/step - loss: 0.1206 - accuracy: 0.9663 - val_loss: 0.0688 - val_accuracy: 0.9753\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 19s 378ms/step - loss: 0.1078 - accuracy: 0.9727 - val_loss: 0.0692 - val_accuracy: 0.9711\n"
     ]
    }
   ],
   "source": [
    "#Due to using the fit_generator we will not be able to use the setting validation_split =.2 as we used earlier. \n",
    "#spliting the data\n",
    "X_train_v2, X_val, y_train_v2, y_val = train_test_split(X_train_v2, y_train_v2, test_size = 0.1, random_state= 2)\n",
    "X_train_v2=X_train_v2/255.0\n",
    "X_val=X_val/255.0\n",
    "\n",
    "\n",
    "#Fitting the model and augmenting the data\n",
    "datagen.fit(X_train_v2)\n",
    "model2 = create_model_v2()\n",
    "history = model2.fit_generator(datagen.flow(X_train_v2,y_train_v2, batch_size=86),\n",
    "                              epochs =  30, \n",
    "                              verbose = 1,\n",
    "                              validation_data = (X_val,y_val),\n",
    "                              steps_per_epoch=X_train_v2.shape[0] // 86,\n",
    "                              callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 900us/step\n",
      "Accuracy of Testing:  [28.417155517578124, 0.9539999961853027]\n"
     ]
    }
   ],
   "source": [
    "#Model Evaluation\n",
    "score_test2 = model2.evaluate(X_test_v2,y_test_v2,  verbose=1)\n",
    "                            \n",
    "print(\"Accuracy of Testing: \",score_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Interpretation of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above we can see that the second model overall had much better results with the test about 95% Accuracy. I will not there were some notable changes in this testing we used a 10% validation set, 0 and 1 were not included in the accecptable characters (ie only the characters that showed up in the 970 examples were utilized.) In addition to the overall model changing considerably there were double the amount of convultion and data was also fited for tilts in each of the charcters of the data set. With the restriction of characters to those found in the train set one may say this would make the general use case not as strong. Further examples and testing could be done to optimize for these or editing of the first model via more edge case, utlizing the same agumentation technique, or also using an optimizer on the first model could result in a better first model and more generalizeable model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model is useless unless you can put it into production. So for this we will need to create a function that we can call and predict  the results from. This can also act as another form of validation. Becuase the first model was faster in creation, the below function was made based on that model. However, this could be easily adapted for the other model and more than one input solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(filelocation):\n",
    "    to_predict = cv2.imread(filelocation, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if to_predict is not None:\n",
    "        to_predict = to_predict/ 255.0\n",
    "\n",
    "    else: \n",
    "        print(\"File not found\")\n",
    "        \n",
    "    res = np.array(model.predict(to_predict[np.newaxis, :, :, np.newaxis]))\n",
    "    ans = np.reshape(res, (5, 36))\n",
    "    \n",
    "    #For each letter prediction we tage the max possibility much like Random Forest\n",
    "    l_ind = []\n",
    "    for a in ans: \n",
    "        l_ind.append(np.argmax(a))\n",
    "\n",
    "    #Output a full captcha instead of a list of the letters predicted.   \n",
    "    capt_ch = ''\n",
    "    for l in l_ind:\n",
    "        capt_ch += symbols[l]\n",
    "        \n",
    "    return capt_ch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22d5n\n",
      "25w53\n",
      "268g2\n",
      "2ycn8\n"
     ]
    }
   ],
   "source": [
    "print(prediction('../Project/train/22d5n.png'))\n",
    "print(prediction('../Project/train/25w53.png'))\n",
    "print(prediction('../Project/train/268g2.png'))\n",
    "print(prediction('../Project/train/2ycn8.png'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Project_Discription.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
