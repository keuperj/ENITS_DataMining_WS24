{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2ter7IP-XOg3"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svWe0YMOXuNY",
        "outputId": "af86d77c-237e-4883-bfbc-73a136d7424c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ],
      "source": [
        "#update gensim\n",
        "!pip install --upgrade gensim #numpy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h1i3V5SXOg7"
      },
      "source": [
        "\n",
        "Doc2Vec Model\n",
        "=============\n",
        "\n",
        "Introduces Gensim's Doc2Vec model and demonstrates its use on the\n",
        "`Lee Corpus <https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`__.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zbQur-t8XOg_"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oho7olrSXOhA"
      },
      "source": [
        "Doc2Vec is a `core_concepts_model` that represents each\n",
        "`core_concepts_document` as a `core_concepts_vector`.  This\n",
        "tutorial introduces the model and demonstrates how to train and assess it.\n",
        "\n",
        "Here's a list of what we'll be doing:\n",
        "\n",
        "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
        "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
        "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
        "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
        "4. Assess the model\n",
        "5. Test the model on the test corpus\n",
        "\n",
        "Review: Bag-of-words\n",
        "--------------------\n",
        "\n",
        ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
        "\n",
        "You may be familiar with the `bag-of-words model\n",
        "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
        "`core_concepts_vector` section.\n",
        "This model transforms each document to a fixed-length vector of integers.\n",
        "For example, given the sentences:\n",
        "\n",
        "- ``John likes to watch movies. Mary likes movies too.``\n",
        "- ``John also likes to watch football games. Mary hates football.``\n",
        "\n",
        "The model outputs the vectors:\n",
        "\n",
        "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
        "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
        "\n",
        "Each vector has 10 elements, where each element counts the number of times a\n",
        "particular word occurred in the document.\n",
        "The order of elements is arbitrary.\n",
        "In the example above, the order of the elements corresponds to the words:\n",
        "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
        "\n",
        "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
        "\n",
        "First, they lose all information about word order: \"John likes Mary\" and\n",
        "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
        "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
        "models consider word phrases of length n to represent documents as\n",
        "fixed-length vectors to capture local word order but suffer from data\n",
        "sparsity and high dimensionality.\n",
        "\n",
        "Second, the model does not attempt to learn the meaning of the underlying\n",
        "words, and as a consequence, the distance between vectors doesn't always\n",
        "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
        "second problem.\n",
        "\n",
        "Review: ``Word2Vec`` Model\n",
        "--------------------------\n",
        "\n",
        "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
        "vector space using a shallow neural network. The result is a set of\n",
        "word-vectors where vectors close together in vector space have similar\n",
        "meanings based on context, and word-vectors distant to each other have\n",
        "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
        "together and ``strong`` and ``Paris`` would be relatively far.\n",
        "\n",
        "Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.\n",
        "\n",
        "With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.\n",
        "But what if we want to calculate a vector for the **entire document**\\ ?\n",
        "We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.\n",
        "However, there is a better way...\n",
        "\n",
        "Introducing: Paragraph Vector\n",
        "-----------------------------\n",
        "\n",
        ".. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.\n",
        "\n",
        "Le and Mikolov in 2014 introduced the `Doc2Vec algorithm <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__,\n",
        "which usually outperforms such simple-averaging of ``Word2Vec`` vectors.\n",
        "\n",
        "The basic idea is: act as if a document has another floating word-like\n",
        "vector, which contributes to all training predictions, and is updated like\n",
        "other word-vectors, but we will call it a doc-vector. Gensim's\n",
        ":py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.\n",
        "\n",
        "There are two implementations:\n",
        "\n",
        "1. Paragraph Vector - Distributed Memory (PV-DM)\n",
        "2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
        "\n",
        ".. Important::\n",
        "  Don't let the implementation details below scare you.\n",
        "  They're advanced material: if it's too much, then move on to the next section.\n",
        "\n",
        "PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training\n",
        "a neural network on the synthetic task of predicting a center word based an\n",
        "average of both context word-vectors and the full document's doc-vector.\n",
        "\n",
        "PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training\n",
        "a neural network on the synthetic task of predicting a target word just from\n",
        "the full document's doc-vector. (It is also common to combine this with\n",
        "skip-gram testing, using both the doc-vector and nearby word-vectors to\n",
        "predict a single target word, but only one at a time.)\n",
        "\n",
        "Prepare the Training and Test Data\n",
        "----------------------------------\n",
        "\n",
        "For this tutorial, we'll be training our model using the `Lee Background\n",
        "Corpus\n",
        "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
        "included in gensim. This corpus contains 314 documents selected from the\n",
        "Australian Broadcasting Corporation’s news mail service, which provides text\n",
        "e-mails of headline stories and covers a number of broad topics.\n",
        "\n",
        "And we'll test our model by eye using the much shorter `Lee Corpus\n",
        "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
        "which contains 50 documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a2HkUp9uXOhY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gensim\n",
        "# Set file names for train and test data\n",
        "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
        "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
        "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtcdVY_eXOhZ"
      },
      "source": [
        "Define a Function to Read and Preprocess Text\n",
        "---------------------------------------------\n",
        "\n",
        "Below, we define a function to:\n",
        "\n",
        "- open the train/test file (with latin encoding)\n",
        "- read the file line-by-line\n",
        "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
        "\n",
        "The file we're reading is a **corpus**.\n",
        "Each line of the file is a **document**.\n",
        "\n",
        ".. Important::\n",
        "  To train the model, we'll need to associate a tag/number with each document\n",
        "  of the training corpus. In our case, the tag is simply the zero-based line\n",
        "  number.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WDbRi3AQXOha"
      },
      "outputs": [],
      "source": [
        "import smart_open\n",
        "\n",
        "def read_corpus(fname, tokens_only=False):\n",
        "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            tokens = gensim.utils.simple_preprocess(line)\n",
        "            if tokens_only:\n",
        "                yield tokens\n",
        "            else:\n",
        "                # For training data, add tags\n",
        "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
        "\n",
        "train_corpus = list(read_corpus(lee_train_file))\n",
        "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQhuIzMsXOhb"
      },
      "source": [
        "Let's take a look at the training corpus\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vrsq3F3XOhc",
        "outputId": "f3ce9a78-350a-41ce-f4da-ce9d8bc8b053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]), TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]\n"
          ]
        }
      ],
      "source": [
        "print(train_corpus[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YD24UUfXOhd"
      },
      "source": [
        "And the testing corpus looks like this:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "559GGBA-XOhe",
        "outputId": "590a0e72-febe-4258-d1d9-bfc48790979c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]\n"
          ]
        }
      ],
      "source": [
        "print(test_corpus[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92q6JFy7XOhe"
      },
      "source": [
        "Notice that the testing corpus is just a list of lists and does not contain\n",
        "any tags.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzVRia6-XOhf"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "\n",
        "Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and\n",
        "iterating over the training corpus 40 times. We set the minimum word count to\n",
        "2 in order to discard words with very few occurrences. (Without a variety of\n",
        "representative examples, retaining such infrequent words can often make a\n",
        "model worse!) Typical iteration counts in the published `Paragraph Vector paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__\n",
        "results, using 10s-of-thousands to millions of docs, are 10-20. More\n",
        "iterations take more time and eventually reach a point of diminishing\n",
        "returns.\n",
        "\n",
        "However, this is a very very small dataset (300 documents) with shortish\n",
        "documents (a few hundred words). Adding training passes can sometimes help\n",
        "with such small datasets.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TCM9vSAKXOhg"
      },
      "outputs": [],
      "source": [
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrwSyuxoXOhg"
      },
      "source": [
        "Build a vocabulary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JpJiP2B7XOhh"
      },
      "outputs": [],
      "source": [
        "model.build_vocab(train_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdXp33oVXOhh"
      },
      "source": [
        "Essentially, the vocabulary is a list (accessible via\n",
        "``model.wv.index_to_key``) of all of the unique words extracted from the training corpus.\n",
        "Additional attributes for each word are available using the ``model.wv.get_vecattr()`` method,\n",
        "For example, to see how many times ``penalty`` appeared in the training corpus:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2ZqUxQlXOhi",
        "outputId": "776d6df7-c882-488e-f548-b9f86cf68079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word 'penalty' appeared 4 times in the training corpus.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Word 'penalty' appeared {model.wv.get_vecattr('penalty', 'count')} times in the training corpus.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEx_s0B6XOhi"
      },
      "source": [
        "Next, train the model on the corpus.\n",
        "If optimized Gensim (with BLAS library) is being used, this should take no more than 3 seconds.\n",
        "If the BLAS library is not being used, this should take no more than 2\n",
        "minutes, so use optimized Gensim with BLAS if you value your time.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SKYhNiTBXOhj"
      },
      "outputs": [],
      "source": [
        "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJjMlORDXOhj"
      },
      "source": [
        "Now, we can use the trained model to infer a vector for any piece of text\n",
        "by passing a list of words to the ``model.infer_vector`` function. This\n",
        "vector can then be compared with other vectors via cosine similarity.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SphLN1maXOhk",
        "outputId": "43a705ff-dc29-4be8-faac-d8d9fb68a998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.2036047  -0.36925006 -0.08059084  0.20516571 -0.16269849 -0.05481001\n",
            "  0.02167912  0.03251893 -0.22423048 -0.15266341  0.13049749 -0.01493027\n",
            "  0.04883939 -0.02212895 -0.10435922 -0.1221455   0.19544579  0.25193962\n",
            "  0.09538864 -0.15704773  0.00273525  0.07597376  0.29359812 -0.04157568\n",
            " -0.03226374  0.0563889  -0.32911354  0.0354265  -0.16941057 -0.10528995\n",
            "  0.35817263  0.07957911  0.15698595  0.12609681  0.17032945  0.13309108\n",
            " -0.00146407 -0.25048593 -0.11509257 -0.05912552  0.04547591  0.04806082\n",
            "  0.08750515 -0.13133608  0.20388609  0.01001689 -0.07408487 -0.00046999\n",
            "  0.09826171 -0.07341963]\n"
          ]
        }
      ],
      "source": [
        "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
        "print(vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHV0_l9rXOhk"
      },
      "source": [
        "Note that ``infer_vector()`` does *not* take a string, but rather a list of\n",
        "string tokens, which should have already been tokenized the same way as the\n",
        "``words`` property of original training document objects.\n",
        "\n",
        "Also note that because the underlying training/inference algorithms are an\n",
        "iterative approximation problem that makes use of internal randomization,\n",
        "repeated inferences of the same text will return slightly different vectors.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_asYQdVeXOhl"
      },
      "source": [
        "Assessing the Model\n",
        "-------------------\n",
        "\n",
        "To assess our new model, we'll first infer new vectors for each document of\n",
        "the training corpus, compare the inferred vectors with the training corpus,\n",
        "and then returning the rank of the document based on self-similarity.\n",
        "Basically, we're pretending as if the training corpus is some new unseen data\n",
        "and then seeing how they compare with the trained model. The expectation is\n",
        "that we've likely overfit our model (i.e., all of the ranks will be less than\n",
        "2) and so we should be able to find similar documents very easily.\n",
        "Additionally, we'll keep track of the second ranks for a comparison of less\n",
        "similar documents.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OyFd4hAsXOhl"
      },
      "outputs": [],
      "source": [
        "ranks = []\n",
        "second_ranks = []\n",
        "for doc_id in range(len(train_corpus)):\n",
        "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
        "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "    rank = [docid for docid, sim in sims].index(doc_id)\n",
        "    ranks.append(rank)\n",
        "\n",
        "    second_ranks.append(sims[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg-i_F-fXOhm"
      },
      "source": [
        "Let's count how each document ranks with respect to the training corpus\n",
        "\n",
        "NB. Results vary between runs due to random seeding and very small corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qo1l9VpbXOhm",
        "outputId": "398100b4-62ea-46c6-c2c2-07290ad2bd25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0: 292, 1: 8})\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "counter = collections.Counter(ranks)\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqByTiT-XOhm"
      },
      "source": [
        "Basically, greater than 95% of the inferred documents are found to be most\n",
        "similar to itself and about 5% of the time it is mistakenly most similar to\n",
        "another document. Checking the inferred-vector against a\n",
        "training-vector is a sort of 'sanity check' as to whether the model is\n",
        "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
        "\n",
        "This is great and not entirely surprising. We can take a look at an example:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOPmnAHkXOhn",
        "outputId": "c06dcb30-fa2b-442d-e8e8-1feb3d751956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document (299): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
            "\n",
            "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>:\n",
            "\n",
            "MOST (299, 0.9479671120643616): «australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as well»\n",
            "\n",
            "SECOND-MOST (104, 0.7853354215621948): «australian cricket captain steve waugh has supported fast bowler brett lee after criticism of his intimidatory bowling to the south african tailenders in the first test in adelaide earlier this month lee was fined for giving new zealand tailender shane bond an unsportsmanlike send off during the third test in perth waugh says tailenders should not be protected from short pitched bowling these days you re earning big money you ve got responsibility to learn how to bat he said mean there no times like years ago when it was not professional and sort of bowlers code these days you re professional our batsmen work very hard at their batting and expect other tailenders to do likewise meanwhile waugh says his side will need to guard against complacency after convincingly winning the first test by runs waugh says despite the dominance of his side in the first test south africa can never be taken lightly it only one test match out of three or six whichever way you want to look at it so there lot of work to go he said but it nice to win the first battle definitely it gives us lot of confidence going into melbourne you know the big crowd there we love playing in front of the boxing day crowd so that will be to our advantage as well south africa begins four day match against new south wales in sydney on thursday in the lead up to the boxing day test veteran fast bowler allan donald will play in the warm up match and is likely to take his place in the team for the second test south african captain shaun pollock expects much better performance from his side in the melbourne test we still believe that we didn play to our full potential so if we can improve on our aspects the output we put out on the field will be lot better and we still believe we have side that is good enough to beat australia on our day he said»\n",
            "\n",
            "MEDIAN (102, 0.2499009519815445): «the secretary general of the law council michael lavarch says the government proposed new asio laws need guarantees to protect the rights of individuals the federal government wants to give officers the power to detain suspects for hours without legal representation asio already has the power to jail people for up to five years if they refuse to answer questions mr lavarch former labor attorney general says he is concerned asio could use the laws to detain people indefinitely the government yet to make its case it very draconian power and if it is required in order to protect public safety it absolutely essential that there be important safeguards he said»\n",
            "\n",
            "LEAST (216, -0.1363389492034912): «senior taliban official confirmed the islamic militia would begin handing over its last bastion of kandahar to pashtun tribal leaders on friday this agreement was that taliban should surrender kandahar peacefully to the elders of these areas and we should guarantee the lives and the safety of taliban authorities and all the taliban from tomorrow should start this program former taliban ambassador to pakistan abdul salam zaeef told cnn in telephone interview he insisted that the taliban would not surrender to hamid karzai the new afghan interim leader and pashtun elder who has been cooperating with the united states to calm unrest among the southern tribes the taliban will surrender to elders not to karzai karzai and other persons which they want to enter kandahar by the support of america they don allow to enter kandahar city he said the taliban will surrender the weapons the ammunition to elders»\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNmiue0WXOhn"
      },
      "source": [
        "Notice above that the most similar document (usually the same text) is has a\n",
        "similarity score approaching 1.0. However, the similarity score for the\n",
        "second-ranked documents should be significantly lower (assuming the documents\n",
        "are in fact different) and the reasoning becomes obvious when we examine the\n",
        "text itself.\n",
        "\n",
        "We can run the next cell repeatedly to see a sampling other target-document\n",
        "comparisons.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJYd5s2OXOhn",
        "outputId": "6c95d367-33b8-4e4b-92d6-0779a120926d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Document (5): «the federal government says it should be safe for afghani asylum seekers in australia to return home when the environment becomes secure the government has suspended their applications while the interim government is established in kabul the foreign affairs minister alexander downer has refused to say for how long the claims process has been put on hold but he says the major threat to most people seeking asylum is no longer there many afghans who have tried to get into australia or for that matter into britain and other countries in north west europe have claimed that they are fleeing the taliban he said well the taliban is no longer in power in afghanistan the taliban is finished meanwhile there has been mass airlift of detainees from christmas island to the pacific island of nauru in total more than people have been flown from the island in two operations using chartered aircraft the second airlift today delivered asylum seekers to nauru where they will await processing of their claims for temporary visas the department of immigration says there are now detainees remaining on christmas island spokesman says decision regarding their future is yet to be made»\n",
            "\n",
            "Similar Document (218, 0.6473293900489807): «refugee support groups are strongly critical of federal government claims that the pacific solution program is working well the immigration minister philip ruddock says he is pleased with the program which uses pacific island nations to process asylum seekers wanting to come to australia president of the hazara ethnic society of australia hassan ghulam says the australian government is bullying smaller nations into accepting asylum seekers if the pacific countries wanted refugees they can clearly raise their voice in the united nations and say yes we are accepting refugees and why australia who gives this authority to the australian government to force the pacific countries to accept refugees in this form or in the other form he asked»\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Pick a random document from the corpus and infer a vector from the model\n",
        "import random\n",
        "doc_id = random.randint(0, len(train_corpus) - 1)\n",
        "\n",
        "# Compare and print the second-most-similar document\n",
        "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
        "sim_id = second_ranks[doc_id]\n",
        "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvHT5WgsXOho"
      },
      "source": [
        "Testing the Model\n",
        "-----------------\n",
        "\n",
        "Using the same approach above, we'll infer the vector for a randomly chosen\n",
        "test document, and compare the document to our model by eye.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0HppMluXOho",
        "outputId": "d0412af4-2bef-4f6d-f7bd-60266c2299ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Document (17): «the united nations world food program estimates that up to million people in seven countries malawi mozambique zambia angola swaziland lesotho and zimbabwe face death by starvation unless there is massive international response in malawi as many as people may have already died the signs of malnutrition swollen stomachs stick thin arms light coloured hair are everywhere»\n",
            "\n",
            "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec<dm/m,d50,n5,w5,mc2,s0.001,t3>:\n",
            "\n",
            "MOST (296, 0.7957379221916199): «today is world aids day and the latest figures show that million people are living with hiv world wide the latest united nations report on the aids epidemic has found eastern europe and the republics of the former soviet union are becoming the new battleground in the fight against the disease un officials say in russia the number of people carrying hiv doubles almost annually while ukraine has become the first nation in europe to report per cent of its adult population is hiv positive the officials say combination of economic insecurity high unemployment and deteriorating health services are behind the steep rise»\n",
            "\n",
            "MEDIAN (78, 0.3984890878200531): «the private business sector has to comply with national privacy laws from today which force them to implement new codes for the handling of personal information the government and the credit sectors are already obliged to protect private data but this is the first time the same rules have been applied to the private sector the new laws are about consent knowledge about data use accuracy and security in health data must only be used for the purpose it is taken for example treatment the federal privacy commissioner malcolm crompton says hospitals and doctors collecting health information now have to ask for consent they re not in position any more of being able to give that information out in an identifiable form to research organisations or to pharmaceutical companies and probably most important of all we now have right of access to that information we can now go in and ask for our medical record and we have the right to see it he said the laws also apply to companies running competitions who collect names and details as they will have to state whether the data is intended for marketing mr crompton says large culture change is happening but it is backed by his enforcement powers often we re able to broker solution without having to use the powers very strongly but think both consumers and organisations should rest assured that we will use the powers under the act if we need to do so but there very clear evidence that in the vast majority of circumstances we wont have to he said»\n",
            "\n",
            "LEAST (37, -0.041209813207387924): «australia quicks and opening batsmen have put the side in dominant position going into day three of the boxing day test match against south africa at the mcg australia is no wicket for only runs shy of south africa after andy bichel earlier starred as the tourists fell for when play was abandoned due to rain few overs short of scheduled stumps yesterday justin langer was not out and matthew hayden the openers went on the attack from the start with langer innings including six fours and hayden eight earlier shaun pollock and nantie haywood launched vital rearguard action to help south africa to respectable first innings total the pair put on runs for the final wicket to help the tourists to the south africans had slumped to for through combination of australia good bowling good fielding and good luck after resuming at for yesterday morning the tourists looked to be cruising as jacques kallis and neil mckenzie added without loss but then bichel suddenly had them reeling after snatching two wickets in two balls first he had jacques kallis caught behind for although kallis could consider himself very unlucky as replays showed his bat was long way from the ball on the next ball bichel snatched sharp return catch to dismiss lance klusener first ball and have shot at hat trick bichel missed out on the hat trick and mark boucher and neil mckenzie again steadied the south african innings adding before the introduction of part timer mark waugh to the attack paid off for australia waugh removed boucher for caught by bichel brett lee then chipped in trapping mckenzie leg before for with perfect inswinger bichel continued his good day in the field running out claude henderson for with direct hit from the in field lee roared in to allan donald bouncing him and then catching the edge with rising delivery which ricky ponting happily swallowed at third slip to remove the returning paceman for duck bichel did not get his hat trick but ended with the best figures of the australian bowlers after also picking up the final wicket of nantie haywood for lee took for and glenn mcgrath for»\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Pick a random document from the test corpus and infer a vector from the model\n",
        "doc_id = random.randint(0, len(test_corpus) - 1)\n",
        "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
        "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
        "\n",
        "# Compare and print the most/median/least similar documents from the train corpus\n",
        "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
        "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
        "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
        "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOjrzLPlXOho"
      },
      "source": [
        "Conclusion\n",
        "----------\n",
        "\n",
        "Let's review what we've seen in this tutorial:\n",
        "\n",
        "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
        "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
        "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
        "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
        "4. Assess the model\n",
        "5. Test the model on the test corpus\n",
        "\n",
        "That's it! Doc2Vec is a great way to explore relationships between documents.\n",
        "\n",
        "Additional Resources\n",
        "--------------------\n",
        "\n",
        "If you'd like to know more about the subject matter of this tutorial, check out the links below.\n",
        "\n",
        "* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_\n",
        "* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_\n",
        "* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_\n",
        "* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__\n",
        "* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "15iNRsmlIZuE"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "gensim_doc2vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}